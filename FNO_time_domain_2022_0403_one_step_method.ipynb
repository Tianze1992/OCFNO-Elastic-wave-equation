{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78eca33f-a56a-4e55-92fb-452fa1fed6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from   utilities3 import *\n",
    "import operator\n",
    "from   functools import reduce\n",
    "from   functools import partial\n",
    "from   timeit import default_timer\n",
    "import scipy.io\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95622d4-9755-42c7-97e5-1f398310e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(30)\n",
    "np.random.seed(30)\n",
    "\n",
    "epochs          = 3000         \n",
    "learning_rate   = 0.002 \n",
    "scheduler_step  = 30\n",
    "scheduler_gamma = 0.1\n",
    "sub             = 1\n",
    "ntrain          = 80\n",
    "ntest           = 20\n",
    "T_in            = 50\n",
    "T_end           = 350\n",
    "S               = 104\n",
    "batch_size      = 1\n",
    "mode1           = 12\n",
    "mode2           = 12\n",
    "width           = 60\n",
    "step            = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d02be8d-c4c7-4564-bbd4-fc8929a04c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "class SpectralConv2d_fast(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d_fast, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1/(self.in_channels*self.out_channels*1000)) # here 10 means the dx and dz\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, \\\n",
    "                                                             self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, \\\n",
    "                                                             self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "        self.weights3 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, \\\n",
    "                                                             self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights4 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, \\\n",
    "                                                             self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft_x = torch.fft.rfft2(x)\n",
    "        x_ft_z = torch.fft.rfft2(torch.transpose(x,2,3))\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft_x = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, \\\n",
    "                             dtype=torch.cfloat, device=x.device)\n",
    "        out_ft_z = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, \\\n",
    "                             dtype=torch.cfloat, device=x.device)\n",
    "        out_ft_x[:, :, :self.modes1, :self.modes2]  = self.compl_mul2d(x_ft_x[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft_x[:, :, -self.modes1:, :self.modes2] = self.compl_mul2d(x_ft_x[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "\n",
    "        out_ft_z[:, :, :self.modes1, :self.modes2]  = self.compl_mul2d(x_ft_z[:, :, :self.modes1, :self.modes2], self.weights3)\n",
    "        out_ft_z[:, :, -self.modes1:, :self.modes2] = self.compl_mul2d(x_ft_z[:, :, -self.modes1:, :self.modes2], self.weights4)\n",
    "\n",
    "        #Return to physical space\n",
    "        out_ft_x = torch.fft.irfft2(out_ft_x, s=(x.size(-2), x.size(-1)))\n",
    "        out_ft_z = torch.fft.irfft2(out_ft_z, s=(x.size(-2), x.size(-1)))\n",
    "        #print(\"this is the shape of the out_x\", out_ft_x.shape)\n",
    "        #print(\"this is the shape of the out_z\", out_ft_z.shape)\n",
    "        \n",
    "        return out_ft_x, out_ft_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b121c290-7d05-4876-bd16-68e865cba296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "\n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.fc0 = nn.Linear((50+5), self.width)\n",
    "        # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv4 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv5 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv6 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "\n",
    "\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w4 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w5 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w6 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.bn0 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn5 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn6 = torch.nn.BatchNorm2d(self.width)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 300)\n",
    "        #self.fc2 = nn.Linear(200, args.step)\n",
    "\n",
    "    def forward(self, x , C1, C2, C3):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        size_x, size_y = x.shape[1], x.shape[2]\n",
    "\n",
    "        C1 = torch.reshape(C1,[batchsize,1,size_x,size_y])\n",
    "        C2 = torch.reshape(C2,[batchsize,1,size_x,size_y])\n",
    "        C3 = torch.reshape(C3,[batchsize,1,size_x,size_y])\n",
    "\n",
    "        C1 = torch.nn.functional.normalize(C1)\n",
    "        C2 = torch.nn.functional.normalize(C2)\n",
    "        C3 = torch.nn.functional.normalize(C3)\n",
    "\n",
    "        C1 = torch.reshape(C1,[batchsize,size_x,size_y,1])\n",
    "        C2 = torch.reshape(C2,[batchsize,size_x,size_y,1])\n",
    "        C3 = torch.reshape(C3,[batchsize,size_x,size_y,1])\n",
    "\n",
    "        grid = self.get_grid(batchsize, size_x, size_y, x.device)\n",
    "        '''\n",
    "        print(\"this is the shape of x\",     x.shape)\n",
    "        print(\"this is the shape of grid \", grid.shape)\n",
    "        print(\"this is the shape of C1 \",   C1.shape)\n",
    "        print(\"this is the shape of C2 \",   C2.shape)\n",
    "        print(\"this is the shape of C3 \",   C3.shape)\n",
    "        '''\n",
    "        x = torch.cat((x, grid, C1, C2, C3), dim=-1)\n",
    "        #print(\"this is the shape od x \",x.shape)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x0, z0 = self.conv0(x)\n",
    "        xw_0   = self.w0(x)\n",
    "        #print(\"this is the shape of the wx0\", xw_0.shape)\n",
    "        x      = self.bn0(xw_0 + x0 + z0)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x1, z1 = self.conv1(x)\n",
    "        xw_1   = self.w1(x)\n",
    "        x      = self.bn1(xw_1 + x1 + z1)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x2, z2 = self.conv2(x)\n",
    "        xw_2   = self.w2(x)\n",
    "        x      = self.bn2(xw_2 + x2 + z2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x3, z3 = self.conv3(x)\n",
    "        xw_3   = self.w3(x)\n",
    "        x      = self.bn3(xw_3 + x3 + z3)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, batchsize, size_x, size_y, device):\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08d78c6-da6e-41ac-94d0-9a260dd840bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_pad(input_tensor,npad, freeSurface,dtype):\n",
    "    \"\"\"\n",
    "    This function is to padding velocity tensor for implementing the absorbing boudary condition.\n",
    "        input_tensor: is a 3D tensor, shape=[1, self.in_channels, self.nz, self.nx]\n",
    "        output_tensor: is also a 3D tensor, shape=[1, self.in_channels, self.nz_pad, self.nx_pad]\n",
    "    \"\"\"\n",
    "    nz,nx = input_tensor.shape[0],input_tensor.shape[1]\n",
    "    input_tensor = input_tensor.reshape(1,1,nz,nx)\n",
    "    #print(\"this is input tensor shape\", input_tensor.shape)\n",
    "\n",
    "    nz_pad = nz + 2* npad\n",
    "    nx_pad = nx + 2* npad\n",
    "    in_channels = 1\n",
    "\n",
    "    if freeSurface:\n",
    "        vpadTop = input_tensor\n",
    "    else:\n",
    "        vtop = torch.ones((1, in_channels, npad, nx),dtype = dtype) * input_tensor[:,:,0,:]\n",
    "        vpadTop = torch.cat((vtop,input_tensor), -2) #padding on axis=2 (nz)\n",
    "\n",
    "    vbottom = torch.ones((1, in_channels, npad, nx),dtype = dtype) * input_tensor[:,:,-1,:]\n",
    "    vpadBottom = torch.cat([vpadTop,vbottom], -2)#padding on axis=2 (nz)\n",
    "\n",
    "    vleft = torch.ones((1, in_channels, nz_pad, npad),dtype = dtype) * \\\n",
    "    vpadBottom[:,:,:,0].view(1, in_channels, -1, 1)\n",
    "    vpadLeft = torch.cat([vleft,vpadBottom], -1)#padding on axis=3 (nx)\n",
    "\n",
    "    vright = torch.ones((1, in_channels, nz_pad, npad),dtype= dtype) * \\\n",
    "    vpadBottom[:,:,:,-1].view(1,in_channels, -1, 1)\n",
    "    output_tensor = torch.cat([vpadLeft,vright], -1)#padding on axis=3 (nx)\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffe5a44-98b0-4ade-b9bb-95738799d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the shape of the original size torch.Size([100, 64, 64])\n",
      "3000 0.002 30 0.1\n",
      "this is the shape of vp vs and rho torch.Size([100, 64, 64, 1]) torch.Size([100, 64, 64, 1]) torch.Size([100, 64, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = 'E:\\FNO_fields\\FN_small_model_various_source'\n",
    "#TRAIN_PATH = '/Users/zhangtianze/Documents/FWI/fourier_neural_operator-master/ns_data.mat'\n",
    "vx_train = torch.load(TRAIN_PATH+'/vx_save_0916_tnesor.pt')\n",
    "#vz_train = torch.load(\"/home/zhang.tianze/fourier_neural_operator-master/vz_save.pt\")\n",
    "\n",
    "vp_index  = torch.load(TRAIN_PATH+'/vp_model_index_0916.pt')\n",
    "vs_index  = torch.load(TRAIN_PATH+'/vs_model_index_0916.pt')\n",
    "rho_index = torch.load(TRAIN_PATH+'/rho_model_index_0916.pt')\n",
    "\n",
    "print(\"this is the shape of the original size\",vp_index.shape)\n",
    "\n",
    "vp_index = vp_index[0:100,:,:].reshape(100,64,64,1)\n",
    "vs_index = vs_index[0:100,:,:].reshape(100,64,64,1)\n",
    "rho_index = rho_index[0:100,:,:].reshape(100,64,64,1)\n",
    "\n",
    "print(epochs, learning_rate, scheduler_step, scheduler_gamma)\n",
    "print(\"this is the shape of vp vs and rho\",vp_index.shape,vs_index.shape,rho_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22428e7d-5a90-4882-bf81-c5ff9edcbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_index_reshape  = torch.zeros(100,104,104)\n",
    "vs_index_reshape  = torch.zeros(100,104,104)\n",
    "rho_index_reshape = torch.zeros(100,104,104)\n",
    "\n",
    "for i_model_index in range(100):\n",
    "    vp_index_reshape_i = tensor_pad(vp_index[i_model_index,:,:,:], 20, False, torch.float32)\n",
    "    vp_index_reshape[i_model_index,:,:] = vp_index_reshape_i.squeeze()\n",
    "\n",
    "    vs_index_reshape_i = tensor_pad(vs_index[i_model_index,:,:,:], 20, False, torch.float32)\n",
    "    vs_index_reshape[i_model_index,:,:] = vs_index_reshape_i.squeeze()\n",
    "\n",
    "    rho_index_reshape_i = tensor_pad(rho_index[i_model_index,:,:,:], 20, False, torch.float32)\n",
    "    rho_index_reshape[i_model_index,:,:] = rho_index_reshape_i.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87dc5312-5e4f-413c-9bc0-fd2f38ada617",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_index_reshape =  vp_index_reshape.reshape(100,104,104,1)\n",
    "vs_index_reshape =  vs_index_reshape.reshape(100,104,104,1)\n",
    "rho_index_reshape = rho_index_reshape.reshape(100,104,104,1)\n",
    "\n",
    "vp_index_reshape =  vp_index_reshape.reshape(100,104,104,1)\n",
    "vs_index_reshape =  vs_index_reshape.reshape(100,104,104,1)\n",
    "rho_index_reshape = rho_index_reshape.reshape(100,104,104,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1091c8e4-8091-4efc-8f05-59fdbd2316df",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = np.zeros(2, )\n",
    "t1 = default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702b7dce-6e60-448c-aac9-8b281618b942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the shape of vx_train torch.Size([100, 800, 104, 104])\n",
      "torch.Size([100, 104, 104, 800])\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the shape of vx_train\", vx_train.shape)\n",
    "#print(\"this is the shape of vx_train\", vz_train.shape)\n",
    "vx_train_reshape = vx_train.permute(0,2,3,1)\n",
    "\n",
    "print(vx_train_reshape.shape)\n",
    "#print(vz_train_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13e8702-8399-452a-88ca-81000dd62624",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = vx_train_reshape[:ntrain,::sub,::sub,:T_in]\n",
    "train_u = vx_train_reshape[:ntrain,::sub,::sub,T_in:T_end]\n",
    "\n",
    "train_vp  =  vp_index_reshape[:ntrain,::sub,::sub,:]\n",
    "train_vs  =  vs_index_reshape[:ntrain,::sub,::sub,:]\n",
    "train_rho = rho_index_reshape[:ntrain,::sub,::sub,:]\n",
    "\n",
    "test_a  = vx_train_reshape[ntrain:ntrain+ntest,::sub,::sub,:T_in]\n",
    "test_u  = vx_train_reshape[ntrain:ntrain+ntest,::sub,::sub,T_in:T_end]\n",
    "\n",
    "test_vp  = vp_index_reshape [ntrain:ntrain+ntest,::sub,::sub,:]\n",
    "test_vs  = vs_index_reshape [ntrain:ntrain+ntest,::sub,::sub,:]\n",
    "test_rho = rho_index_reshape[ntrain:ntrain+ntest,::sub,::sub,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d6682c-8b8e-41bf-ac3d-90f6cc072e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the training_set of of a torch.Size([80, 104, 104, 50])\n",
      "this is the training_set of of u torch.Size([80, 104, 104, 300])\n",
      "this is the training_set of of vp torch.Size([80, 104, 104, 1])\n",
      "this is the training_set of of vs torch.Size([80, 104, 104, 1])\n",
      "this is the training_set of of rho torch.Size([80, 104, 104, 1])\n",
      "this is the testing_set of of a torch.Size([20, 104, 104, 50])\n",
      "this is the testing_set of of u torch.Size([20, 104, 104, 300])\n",
      "this is the testing_set of of vp torch.Size([20, 104, 104, 1])\n",
      "this is the testing_set of of vs torch.Size([20, 104, 104, 1])\n",
      "this is the testing_set of of rho torch.Size([20, 104, 104, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the training_set of of a\", train_a.shape)\n",
    "print(\"this is the training_set of of u\", train_u.shape)\n",
    "print(\"this is the training_set of of vp\",  train_vp.shape)\n",
    "print(\"this is the training_set of of vs\",  train_vs.shape)\n",
    "print(\"this is the training_set of of rho\", train_rho.shape)\n",
    "\n",
    "print(\"this is the testing_set of of a\", test_a.shape)\n",
    "print(\"this is the testing_set of of u\", test_u.shape)\n",
    "\n",
    "print(\"this is the testing_set of of vp\",  test_vp.shape)\n",
    "print(\"this is the testing_set of of vs\",  test_vs.shape)\n",
    "print(\"this is the testing_set of of rho\", test_rho.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d16b38-60b4-4a21-a5ea-6fa8845a66be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 104, 104, 300])\n"
     ]
    }
   ],
   "source": [
    "print(train_u.shape)\n",
    "assert (S            == train_u.shape[-2])\n",
    "assert ((T_end-T_in) == train_u.shape[-1])\n",
    "\n",
    "assert (S            == test_u.shape[-2])\n",
    "assert ((T_end-T_in) == test_u.shape[-1])\n",
    "\n",
    "assert (S == train_vp.shape[-2])\n",
    "assert (S == train_vs.shape[-2])\n",
    "assert (S == train_rho.shape[-2])\n",
    "\n",
    "assert (S == test_vp.shape[-2])\n",
    "assert (S == test_vs.shape[-2])\n",
    "assert (S == test_rho.shape[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c11694-89a9-4909-a650-32bc381a0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = train_a.reshape(ntrain, S, S,T_in)\n",
    "train_u = train_u.reshape(ntrain, S, S,(T_end - T_in))\n",
    "test_a  = test_a.reshape( ntest,  S, S,T_in)\n",
    "test_u  = test_u.reshape( ntest,  S, S,(T_end - T_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18716432-549d-4e34-a4c5-0a4de78923ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u,\\\n",
    "                                                                          train_vp,train_vs,train_rho), \\\n",
    "                                                                          batch_size = batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u, \\\n",
    "                                                                          test_vp,test_vs,test_rho), \\\n",
    "                                                                          batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76eb10b3-43b1-452a-bb3e-1f493d65d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the devide we are using device \n",
      " cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"this is the devide we are using device \\n\", device)\n",
    "model = FNO2d(mode1, mode2, width).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b1d69b-b432-47c6-bf57-d159e0581489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14563320\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3244800) must match the size of tensor b (10816) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m im_train \u001b[38;5;241m=\u001b[39m model(xx,C1,C2,C3)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#loss += costFunc(im_train,y)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmyloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m     pred \u001b[38;5;241m=\u001b[39m im_train\n",
      "File \u001b[1;32m~\\utilities3.py:204\u001b[0m, in \u001b[0;36mLpLoss.__call__\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\utilities3.py:192\u001b[0m, in \u001b[0;36mLpLoss.rel\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrel\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m    190\u001b[0m     num_examples \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 192\u001b[0m     diff_norms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    193\u001b[0m     y_norms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(y\u001b[38;5;241m.\u001b[39mreshape(num_examples,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3244800) must match the size of tensor b (10816) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "print(count_params(model))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "training_loss_tensor = torch.zeros(epochs)\n",
    "testing_loss_tensor  = torch.zeros(epochs)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "costFunc = torch.nn.MSELoss(reduction='sum')\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2_step_training = 0\n",
    "    test_l2_step_testing = 0\n",
    "    for xx, yy, C1, C2, C3 in train_loader:\n",
    "        #print(\"this the the loading of training loader\")\n",
    "        loss = 0\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        C1 = C1.to(device)\n",
    "        C2 = C2.to(device)\n",
    "        C3 = C3.to(device)\n",
    "\n",
    "        for t in range(0, (T_end - T_in),step):\n",
    "            y = yy[..., t:t + step]\n",
    "            #print(\"this is the shape of y\",y.shape)\n",
    "            im_train = model(xx,C1,C2,C3)\n",
    "            #loss += costFunc(im_train,y)\n",
    "            loss += myloss(im_train.reshape(batch_size, -1), y.reshape(batch_size, -1))\n",
    "            if t == 0:\n",
    "                pred = im_train\n",
    "            else:\n",
    "                pred = torch.cat((pred, im_train), -1)\n",
    "\n",
    "            xx = torch.cat((xx[..., step:], im_train), dim=-1)\n",
    "\n",
    "            #print(\"this is the shape of xx ======>\", xx.shape)\n",
    "        #print(\"this before loss backpropagation\")\n",
    "        train_l2_step_training += loss.item()\n",
    "        #l2_full = myloss(pred.reshape(batch_size, -1), yy.reshape(batch_size, -1))\n",
    "        #train_l2_full += l2_full.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(\"this is the loss ====== >\", loss)\n",
    "        #print(\"this is the epoch====== >\", ep)\n",
    "        optimizer.step()\n",
    "        training_loss_tensor[ep] = train_l2_step_training\n",
    "\n",
    "    test_l2_step = 0\n",
    "    test_l2_full = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xx, yy, C1, C2, C3 in test_loader:\n",
    "            loss = 0\n",
    "            xx = xx.to(device)\n",
    "            yy = yy.to(device)\n",
    "            C1 = C1.to(device)\n",
    "            C2 = C2.to(device)\n",
    "            C3 = C3.to(device)\n",
    "\n",
    "            for t in range(0, (T_end - T_in),step):\n",
    "                y = yy[..., t:t + step]\n",
    "                im_test = model(xx,C1,C2,C3)\n",
    "                #loss += costFunc(im_test,y)\n",
    "                loss += myloss(im_test.reshape(batch_size, -1), y.reshape(batch_size, -1))\n",
    "                if t == 0:\n",
    "                    pred = im_test\n",
    "                else:\n",
    "                    pred = torch.cat((pred, im_test), -1)\n",
    "\n",
    "                xx = torch.cat((xx[..., step:], im_test), dim=-1)\n",
    "\n",
    "            test_l2_step_testing += loss.item()\n",
    "            #test_l2_full += myloss(pred.reshape(batch_size, -1), yy.reshape(batch_size, -1)).item()\n",
    "            testing_loss_tensor[ep] = test_l2_step_testing\n",
    "\n",
    "    t2 = default_timer()\n",
    "    #scheduler.step()\n",
    "    print(\"current epock\",ep, \\\n",
    "          \"processing time in each iteration [{:.4e}]\".format(t2 - t1), \\\n",
    "          \"training loss [{:.4e}]\".format(train_l2_step_training), \\\n",
    "          \"testing loss  [{:.4e}]\".format(test_l2_step_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb6fdd-157f-4a18-b91a-639d4673659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_plot_train  = im_train[2,:,:,200].cpu().clone().detach().numpy()\n",
    "im_plot_tests  = im_test [2,:,:,200].cpu().clone().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ce41b-c6b7-4887-8c58-f87e78485cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10)) # create the canvas for plotting\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.imshow(im_plot_train, cmap='bwr_r')\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.imshow(im_plot_tests, cmap='bwr_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70317a1e-a752-4ff8-b628-702a5f4cdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss_tensor)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cabc9-69ab-45c9-aad3-c200a4575fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(testing_loss_tensor)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622cc4a-7510-4570-879d-81f935aba02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
